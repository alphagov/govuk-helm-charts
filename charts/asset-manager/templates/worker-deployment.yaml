apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}-worker
  labels:
    {{- include "asset-manager.labels" . | nindent 4 }}
    app.kubernetes.io/component: worker
    app: {{ .Release.Name }}-worker
spec:
  replicas: {{ .Values.workerReplicaCount }}
  revisionHistoryLimit: 5
  selector:
    matchLabels:
      app: {{ .Release.Name }}-worker
  template:
    metadata:
      labels:
        {{- include "asset-manager.labels" . | nindent 8 }}
        app.kubernetes.io/component: worker
        app: {{ .Release.Name }}-worker
    spec:
      automountServiceAccountToken: false
      securityContext:
        runAsUser: 1001
        runAsGroup: 1001
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: app
          image: "{{ required "A valid .Values.appImage.repository entry required!" .Values.appImage.repository }}:{{ .Values.appImage.tag }}"
          imagePullPolicy: {{ .Values.appImage.pullPolicy | default "Always" }}
          command: ["bundle"]
          args: ["exec", "sidekiq", "-C", "config/sidekiq.yml"]
          ports:
            - name: metrics
              containerPort: {{ .Values.metricsPort }}
          volumeMounts:
            - name: asset-manager-efs
              mountPath: &uploads-path /mnt/asset-manager
            - name: etc-clamav
              mountPath: /etc/clamav
          envFrom:
            - configMapRef:
                name: govuk-apps-env
          env:
            - name: GOVUK_UPLOADS_ROOT
              value: *uploads-path
            - name: SENTRY_RELEASE
              value: "{{ .Values.appImage.tag }}"
          {{- with .Values.extraEnv }}
            {{- (tpl (toYaml .) $) | trim | nindent 12 }}
          {{- end }}
          {{- with .Values.workerResources }}
          resources:
            {{- . | toYaml | trim | nindent 12 }}
          {{- end }}
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            # Asset Manager needs to run as 2899 since it shares an NFS volume
            # with its EC2 counterpart.
            runAsUser: 2899
            runAsGroup: 2899
        - name: clamd
          image: "{{ required "A valid .Values.appImage.repository entry required!" .Values.appImage.repository }}:{{ .Values.appImage.tag }}"
          imagePullPolicy: {{ .Values.appImage.pullPolicy | default "Always" }}
          command: ["clamd"]
          args: ["--foreground"]
          {{- with .Values.clamdResources }}
          resources:
            {{- . | toYaml | trim | nindent 12 }}
          {{- end }}
          volumeMounts:
            - name: asset-manager-efs
              mountPath: *uploads-path
            - name: clam-virus-db
              mountPath: /var/lib/clamav
            - name: etc-clamav
              mountPath: /etc/clamav
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
      {{- with .Values.dnsConfig }}
      dnsConfig:
        {{- . | toYaml | trim | nindent 8 }}
      {{- end }}
      enableServiceLinks: false
      volumes:
      - name: asset-manager-efs
        nfs:
          server: "{{ .Values.assetManagerNFS }}"
          path: /asset-manager
      - name: clam-virus-db
        nfs:
          server: "{{ .Values.assetManagerNFS }}"
          path: /clamav-db
          readOnly: true
      - name: etc-clamav
        configMap:
          name: {{ .Release.Name }}-etc-clamav
