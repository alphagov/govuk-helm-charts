apiVersion: v1
kind: ConfigMap
metadata:
  name: router-nginx-conf
  labels:
    {{- include "govuk-apps-conf.labels" . | nindent 4 }}
    app.kubernetes.io/component: web
data:
  nginx.conf: |-
    user nginx;

    load_module /usr/lib/nginx/modules/ngx_http_perl_module.so;

    error_log  /var/log/nginx/error.log warn;
    pid        /var/run/nginx.pid;

    events {
      worker_connections  1024;
    }

    http {
      include       /etc/nginx/mime.types;
      default_type  application/octet-stream;

      server_tokens off;

      sendfile        on;
      keepalive_timeout  65;

      perl_set $uri_lowercase 'sub {
        my $r = shift;
        return lc($r->uri);
      }';

      upstream router {
        server 127.0.0.1:3000;
      }

      server {
        listen 8080;

        location / {
          proxy_set_header   Host $http_host;
          proxy_set_header   X-Real-IP $remote_addr;
          proxy_pass         http://router;
          proxy_redirect     off;
        }
        location /assets {
          proxy_set_header   Authorization "";
          proxy_set_header   Connection "";
          proxy_set_header   X-Real-IP $remote_addr;  # TODO: pass the actual end-client address
          proxy_hide_header  x-amz-id-2;
          proxy_hide_header  x-amz-meta-server-side-encryption;
          proxy_hide_header  x-amz-request-id;
          proxy_hide_header  x-amz-server-side-encryption;
          proxy_hide_header  x-amz-version-id;
          add_header         Cache-Control max-age=31536000;
          proxy_intercept_errors on;
          proxy_pass         https://govuk-app-assets-{{ .Values.govukEnvironment }}.s3.eu-west-1.amazonaws.com;
        }
        location = /__canary__ {
          return 200 '{"message": "Tweet tweet"}';
        }
        location ~ ^\/[A-Z]+[A-Z\W\d]+$ {
          rewrite ^(.*)$ $scheme://$host$uri_lowercase permanent;
        }
        location = /robots.txt {
          root /usr/share/nginx/html;
        }
      }
    }
  robots.txt: |-
    {{- if eq .Values.govukEnvironment "staging" }}
    User-agent: *
    Disallow: /
    {{- else }}
    User-agent: *
    Disallow: /*/print$
    # Don't allow indexing of user needs pages
    Disallow: /info/*
    Sitemap: https://www.gov.uk/sitemap.xml

    # https://ahrefs.com/robot/ crawls the site frequently
    User-agent: AhrefsBot
    Crawl-delay: 10

    # https://www.deepcrawl.com/bot/ makes lots of requests. Ideally
    # we'd slow it down rather than blocking it but it doesn't mention
    # whether or not it supports crawl-delay.
    User-agent: deepcrawl
    Disallow: /

    # Complaints of 429 'Too many requests' seem to be coming from SharePoint servers
    # (https://social.msdn.microsoft.com/Forums/en-US/3ea268ed-58a6-4166-ab40-d3f4fc55fef4)
    # The robot doesn't recognise its User-Agent string, see the MS support article:
    # https://support.microsoft.com/en-us/help/3019711/the-sharepoint-server-crawler-ignores-directives-in-robots-txt
    User-agent: MS Search 6.0 Robot
    Disallow: /

    # Google's crawler was sending requests for each variation of query param for the sectors page of licence-finder
    # resulting in millions of requests a day.
    User-agent: Googlebot
    Disallow: /licence-finder/*
    {{- end }}
